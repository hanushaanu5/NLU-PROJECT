{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LGhxUF1SmPcW"
   },
   "source": [
    "# 3-Layer LSTM model with Penn Treebank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IRr388TrNp-T"
   },
   "source": [
    "### Import packages and activate cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jODfPA2GJrDh"
   },
   "outputs": [],
   "source": [
    "import torch, math, time, os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import Variable, Function\n",
    "from torch.nn import Module, Parameter\n",
    "from torch.nn.functional import softmax\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iN3aYwGlrfh3"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "S1-C_PfvNoma",
    "outputId": "34edcc91-2876-4f40-e945-aa4723a8eb1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GPU.\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"Running GPU.\") if use_cuda else print(\"No GPU available.\")\n",
    "\n",
    "\n",
    "def get_variable(x):\n",
    "    \"\"\" Converts tensors to cuda, if available. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cuda()\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_numpy(x):\n",
    "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cpu().data.numpy()\n",
    "    return x.data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Qrlck8zNdCx"
   },
   "source": [
    "### Small function and class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "elybdMT8ppif"
   },
   "outputs": [],
   "source": [
    "#A modification of the simoid function, as described in the article. a defines the slope hard_sigm(x) = max(0,min(1,(ax+1)/2))\n",
    "\n",
    "def hard_sigm(a, x):\n",
    "    temp = torch.div(torch.add(torch.mul(x, a), 1), 2.0)\n",
    "    output = torch.clamp(temp, min=0, max=1)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ZNHyDOHLB7z"
   },
   "outputs": [],
   "source": [
    "class bound(Function):\n",
    "    def forward(self, x):\n",
    "        # forward : x -> output\n",
    "        self.save_for_backward(x)\n",
    "        output = x > 0.5\n",
    "        return output.float()\n",
    "\n",
    "    def backward(self, output_grad):\n",
    "        # backward: output_grad -> x_grad\n",
    "        x = self.saved_tensors\n",
    "        x_grad = None\n",
    "\n",
    "        if self.needs_input_grad[0]:\n",
    "            x_grad = output_grad.clone()\n",
    "\n",
    "        return x_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "acEILFS3LFyW"
   },
   "source": [
    "## LSTM network defined by 3 classes (CELL, HMLSTM AND HMLSTM NET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xY2dlvpuHltx"
   },
   "outputs": [],
   "source": [
    "class HM_LSTMCell(Module):\n",
    "    def __init__(self, bottom_size, hidden_size, top_size, a, last_layer):\n",
    "        super(HM_LSTMCell, self).__init__()\n",
    "        self.bottom_size = bottom_size\n",
    "        self.hidden_size = hidden_size \n",
    "        self.top_size = top_size\n",
    "        self.a = a #se slope annealing trick i training\n",
    "        self.last_layer = last_layer\n",
    "\n",
    "\n",
    "        #Initialize weight matrices for transition of hidden states between HM_LSTM cells\n",
    "        '''\n",
    "        U_recur means the state transition parameters from layer l (current layer) to layer l\n",
    "        U_topdown means the state transition parameters from layer l+1 (top layer) to layer l\n",
    "        W_bottomup means the state transition parameters from layer l-1 (bottom layer) to layer l\n",
    "        '''\n",
    "        self.W_bottomup = Parameter(torch.cuda.FloatTensor(4 * self.hidden_size + 1, self.bottom_size))\n",
    "        self.U_recur = Parameter(torch.cuda.FloatTensor(4 * self.hidden_size + 1, self.hidden_size))\n",
    "\n",
    "        if not self.last_layer:\n",
    "            self.U_topdown = Parameter(torch.cuda.FloatTensor(4 * self.hidden_size + 1, self.top_size))\n",
    "\n",
    "        self.bias = Parameter(torch.cuda.FloatTensor(4 * self.hidden_size + 1))\n",
    "\n",
    "        #Perform weight initialization of these 4 (or 3) parameters with function defined below.\n",
    "        self.reset_parameters()\n",
    "\n",
    "    #Think about redoing weight initialization: https://erwanscornet.github.io/teaching/RNN.pdf\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for par in self.parameters():\n",
    "            par.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, c, h_bottomup, h_recur, h_topdown, z, z_bottom): \n",
    "        # h_bottom.size = bottom_size * batch_size\n",
    "        \"\"\"\n",
    "        c:                  cell state in previous cell (l,t-1)\n",
    "        h_bottomup:         hidden states in layer below (l-1,t)\n",
    "        h_recur:            hidden states in previous timestep (l,t-1)\n",
    "        h_topdown:          hidden states in layer above and previous timestep (l+1,t-1)\n",
    "        z:                  boundary state in previous time step (l,t-1)\n",
    "        z_bottom:           boundary state in layer below (l-1,t)\n",
    "\n",
    "        \"\"\"\n",
    "        #Calculate s-matrices to calculate new cell state (COPY,UPDATE or FLUSH)\n",
    "        s_recur = torch.mm(self.U_recur, h_recur)\n",
    "        \n",
    "        s_bottomup_init = torch.mm(self.W_bottomup, h_bottomup)\n",
    "        s_bottomup = z_bottom.expand_as(s_bottomup_init) * s_bottomup_init\n",
    "\n",
    "        #If not last layer, calculate s_topdown, else set s_topdown to 0\n",
    "        if not self.last_layer:\n",
    "            s_topdown_init = torch.mm(self.U_topdown, h_topdown) \n",
    "            s_topdown = z.expand_as(s_topdown_init) * s_topdown_init\n",
    "        else:\n",
    "            s_topdown = Variable(torch.zeros(s_recur.size()).cuda(), requires_grad=False).cuda()\n",
    "\n",
    "\n",
    "        #Extract individual variables (matrix/vector) from the large S matrix (f_s: f_slice) \n",
    "        f_s = s_recur + s_topdown + s_bottomup + self.bias.unsqueeze(1).expand_as(s_recur)\n",
    "\n",
    "        # f_s.size = (4 * hidden_size + 1) * batch_size\n",
    "        f = torch.sigmoid(f_s[0:self.hidden_size, :])  # hidden_size * batch_size\n",
    "        i = torch.sigmoid(f_s[self.hidden_size:self.hidden_size*2, :])\n",
    "        o = torch.sigmoid(f_s[self.hidden_size*2:self.hidden_size*3, :])\n",
    "        g = torch.tanh(f_s[self.hidden_size*3:self.hidden_size*4, :])\n",
    "        z_hat = hard_sigm(self.a, f_s[self.hidden_size*4:self.hidden_size*4+1, :])\n",
    "\n",
    "        #Make vector of ones and resize the boundary states (z-values) to be the same size as the cell parameters (f, i, o and g) \n",
    "        one = Variable(torch.ones(f.size()).cuda(), requires_grad=False)\n",
    "        z = z.expand_as(f)\n",
    "        z_bottom = z_bottom.expand_as(f)\n",
    "\n",
    "        #Calculate cell state (one line implementation of out commented if/else below)\n",
    "        c_new = z * (i * g) + (one - z) * (one - z_bottom) * c + (one - z) * z_bottom * (f * c + i * g) #burde ikke beregnes for ethvert tilfælde? \n",
    "        h_new = z * o * torch.tanh(c_new) + (one - z) * (one - z_bottom) * h_recur + (one - z) * z_bottom * o * torch.tanh(c_new)\n",
    "\n",
    "        # if z == 1: (FLUSH)\n",
    "        #     c_new = i * g\n",
    "        #     h_new = o * Func.tanh(c_new)\n",
    "        # elif z_bottom == 0: (COPY)\n",
    "        #     c_new = c\n",
    "        #     h_new = h\n",
    "        # else: (UPDATE)\n",
    "        #     c_new = f * c + i * g\n",
    "        #     h_new = o * Func.tanh(c_new)\n",
    "\n",
    "        #Saves original x values: both 0, 1 and values in between (found using a), to be used in the backward pass. \n",
    "        #Hereafter sets x < 0.5 to 0 and x > 0.5 to 1 as they are used in the forward pass\n",
    "        z_new = bound()(z_hat)\n",
    "\n",
    "        return h_new, c_new, z_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DkAV7YKQHm3W"
   },
   "outputs": [],
   "source": [
    "class HM_LSTM(Module): #Istedet for at lave egen LSTM så brug den fra main.py\n",
    "    def __init__(self, a, input_size, size_list):\n",
    "        super(HM_LSTM, self).__init__()\n",
    "        self.a = a\n",
    "        self.input_size = input_size #bottom_size\n",
    "        self.size_list = size_list \n",
    "        \n",
    "        \"\"\"\n",
    "        input_size:        Input size to network\n",
    "        size_list[0]:      hidden size of layer 1 aka. input size after embedding\n",
    "        size_list[1]:      hidden size of layer 2\n",
    "        size_list[2]:      hidden size of layer 3\n",
    "        \"\"\"\n",
    "\n",
    "        #Init cell state\n",
    "        self.cell_1 = HM_LSTMCell(self.input_size, self.size_list[0], self.size_list[1], self.a, False) #bottom_size, hidden_size, top_size, a, last_layer\n",
    "        self.cell_2 = HM_LSTMCell(self.size_list[0], self.size_list[1], self.size_list[2], self.a, False)\n",
    "        self.cell_3 = HM_LSTMCell(self.size_list[1], self.size_list[2], None, self.a, True)\n",
    "\n",
    "    def forward(self, inputs, hidden): #hidden state supplied, if the model is going to freestyle or make predictions that dont start from 0\n",
    "        # inputs.size = (batch_size, time steps, embed_size/input_size) \n",
    "\n",
    "        time_steps = inputs.size(1)\n",
    "        batch_size = inputs.size(0)\n",
    "\n",
    "        if hidden == None:\n",
    "            #Layer 1\n",
    "            h_t1 = Variable(torch.zeros(self.size_list[0], batch_size).float().cuda(), requires_grad=False)\n",
    "            c_t1 = Variable(torch.zeros(self.size_list[0], batch_size).float().cuda(), requires_grad=False)\n",
    "            z_t1 = Variable(torch.zeros(1, batch_size).float().cuda(), requires_grad=False)\n",
    "            #Layer 2\n",
    "            h_t2 = Variable(torch.zeros(self.size_list[1], batch_size).float().cuda(), requires_grad=False)\n",
    "            c_t2 = Variable(torch.zeros(self.size_list[1], batch_size).float().cuda(), requires_grad=False)\n",
    "            z_t2 = Variable(torch.zeros(1, batch_size).float().cuda(), requires_grad=False)\n",
    "            #Layer 3\n",
    "            h_t3 = Variable(torch.zeros(self.size_list[2], batch_size).float().cuda(), requires_grad=False)\n",
    "            c_t3 = Variable(torch.zeros(self.size_list[2], batch_size).float().cuda(), requires_grad=False)\n",
    "            z_t3 = Variable(torch.zeros(1, batch_size).float().cuda(), requires_grad=False)\n",
    "        else:\n",
    "            (h_t1, c_t1, z_t1, h_t2, c_t2, z_t2, h_t3, c_t3, z_t3) = hidden\n",
    "\n",
    "        #Make vector of ones with the same size as batch, so that input is always passed via s_bottomup (see HMLSTMCell)\n",
    "        z_one = Variable(torch.ones(1, batch_size).float().cuda(), requires_grad=False)\n",
    "        \n",
    "        \"\"\"\n",
    "        h_t1:     hidden states of layer 1 at previous timestep (updates to current in each iteration)\n",
    "        h_t2:     hidden states of layer 2 at previous timestep (updates to current timestep in each iteration)\n",
    "        h_t3:     hidden states of layer 3 --||--\n",
    "        z_1:      boundary state of layer 1 --||--\n",
    "        z_2:      boundary state of layer 2 --||--\n",
    "        z_3:      boundary state of layer 3 --||--\n",
    "        \"\"\"\n",
    "\n",
    "        h_1, h_2, h_3, z_1, z_2, z_3 = [], [], [], [], [], [] \n",
    "\n",
    "\n",
    "        for t in range(time_steps):\n",
    "            h_t1, c_t1, z_t1 = self.cell_1(c=c_t1, h_bottomup=inputs[:,t, :].t(), h_recur=h_t1, h_topdown=h_t2, z=z_t1, z_bottom=z_one) #t1 og t2 is layer1 and layer2 and not time1 and time2\n",
    "            h_t2, c_t2, z_t2 = self.cell_2(c=c_t2, h_bottomup=h_t1, h_recur=h_t2, h_topdown=h_t3, z=z_t2, z_bottom=z_t1)  \n",
    "            h_t3, c_t3, z_t3 = self.cell_3(c=c_t3, h_bottomup=h_t2, h_recur=h_t3, h_topdown=None, z=z_t3, z_bottom=z_t2)  \n",
    "\n",
    "            h_1 += [h_t1.t()]\n",
    "            h_2 += [h_t2.t()]\n",
    "            h_3 += [h_t3.t()]\n",
    "            z_1 += [z_t1.t()]\n",
    "            z_2 += [z_t2.t()]\n",
    "            z_3 += [z_t2.t()]\n",
    "\n",
    "        hidden = (h_t1, c_t1, z_t1, h_t2, c_t2, z_t2, h_t3, c_t3, z_t3)\n",
    "        return torch.stack(h_1, dim=1), torch.stack(h_2, dim=1), torch.stack(h_3, dim=1), torch.stack(z_1, dim=1), torch.stack(z_2, dim=1), torch.stack(z_3, dim=1), hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gc2G--sAZoSx"
   },
   "outputs": [],
   "source": [
    "class HM_Net(Module):\n",
    "    def __init__(self, a, size_list, dict_size, embed_size):\n",
    "        super(HM_Net, self).__init__()\n",
    "        self.dict_size = dict_size # Vocab size\n",
    "        self.size_list = size_list # Number of hidden units in each layer\n",
    "        self.embed_in = nn.Embedding(dict_size, embed_size) #dict_size, embed_size\n",
    "        self.HM_LSTM = HM_LSTM(a, embed_size, size_list)\n",
    "        self.weight = nn.Linear(size_list[0]+size_list[1]+size_list[2],3)\n",
    "        self.embed_out1 = nn.Linear(size_list[0], dict_size)\n",
    "        self.embed_out2 = nn.Linear(size_list[1], dict_size)\n",
    "        self.embed_out3 = nn.Linear(size_list[2], dict_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, inputs, target, hidden):\n",
    "        # inputs : batch_size * time_steps\n",
    "  \n",
    "        #Embedding\n",
    "        emb = self.embed_in(inputs)  # batch_size * time_steps * embed_size\n",
    "\n",
    "        #Initialize network and do forward pass\n",
    "        h_1, h_2, h_3, z_1, z_2, z_3, hidden = self.HM_LSTM(emb, hidden)  # batch_size * time_steps * hidden_size\n",
    "\n",
    "        h = torch.cat((h_1, h_2, h_3), 2) #Applied as a 3D tensor. \n",
    "\n",
    "\n",
    "        #Do FFNN throuch weights g1 and g2 to h_e which acts as predictor of x_(t+1). \n",
    "        #h: Dim: batch, seq len,  total units in hidden layer\n",
    "        g = torch.sigmoid(self.weight(h.view(h.size(0)*h.size(1), h.size(2)))) # This g is not the LSTM g, but the g combining the different hidden states to predict the outputs\n",
    "        g_1 = g[:, 0:1]  # batch_size * time_steps, 1\n",
    "        g_2 = g[:, 1:2]\n",
    "        g_3 = g[:, 2:3]\n",
    "\n",
    "        h_e1 = g_1.expand(g_1.size(0), self.dict_size)*self.embed_out1(h_1.view(h_1.size(0)*h_1.size(1), h_1.size(2)))  \n",
    "        h_e2 = g_2.expand(g_2.size(0), self.dict_size)*self.embed_out2(h_2.view(h_2.size(0)*h_2.size(1), h_2.size(2)))\n",
    "        h_e3 = g_3.expand(g_3.size(0), self.dict_size)*self.embed_out3(h_3.view(h_3.size(0)*h_3.size(1), h_3.size(2)))\n",
    "\n",
    "        h_e = self.relu(h_e1 + h_e2 + h_e3)  # batch_size*time_steps, hidden_size\n",
    "        output = h_e\n",
    "\n",
    "        batch_loss = self.loss(output, target) # Variable(target)\n",
    "\n",
    "        return batch_loss, hidden, output, z_1, z_2, z_3\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        #Layer 1\n",
    "        h_t1 = Variable(torch.zeros(self.size_list[0], batch_size).float().cuda(), requires_grad=False)\n",
    "        c_t1 = Variable(torch.zeros(self.size_list[0], batch_size).float().cuda(), requires_grad=False)\n",
    "        z_t1 = Variable(torch.zeros(1, batch_size).float().cuda(), requires_grad=False)\n",
    "        # Layer 2\n",
    "        h_t2 = Variable(torch.zeros(self.size_list[1], batch_size).float().cuda(), requires_grad=False)\n",
    "        c_t2 = Variable(torch.zeros(self.size_list[1], batch_size).float().cuda(), requires_grad=False)\n",
    "        z_t2 = Variable(torch.zeros(1, batch_size).float().cuda(), requires_grad=False)\n",
    "        # Layer 3\n",
    "        h_t3 = Variable(torch.zeros(self.size_list[2], batch_size).float().cuda(), requires_grad=False)\n",
    "        c_t3 = Variable(torch.zeros(self.size_list[2], batch_size).float().cuda(), requires_grad=False)\n",
    "        z_t3 = Variable(torch.zeros(1, batch_size).float().cuda(), requires_grad=False)\n",
    "\n",
    "        hidden = (h_t1, c_t1, z_t1, h_t2, c_t2, z_t2, h_t3, c_t3, z_t3 )\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bhlXubs4LW0A"
   },
   "source": [
    "### Load data and activate cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "qFN-uXHkuXfa",
    "outputId": "9acc54fc-9779-4795-eefe-4ec4032a6ab2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.fields {'text': <torchtext.data.field.Field object at 0x7f20d7d0b7b8>}\n",
      "len(train) 1\n",
      "vars(train[0]) [' ', 'a', 'e', 'r', ' ', 'b', 'a', 'n', 'k', 'n']\n",
      "len(TEXT.vocab) 51\n"
     ]
    }
   ],
   "source": [
    "#Tokenizer: splits sentences into char tokens\n",
    "def tokenize(lines):\n",
    "    return [line for line in lines]\n",
    "\n",
    "# set up fields\n",
    "TEXT = data.Field(lower=True, tokenize=tokenize, batch_first=True)\n",
    "\n",
    "# make splits for data\n",
    "train, valid, test = datasets.PennTreebank.splits(TEXT)\n",
    "\n",
    "# print information about the data\n",
    "print('train.fields', train.fields)\n",
    "print('len(train)', len(train))\n",
    "print('vars(train[0])', vars(train[0])['text'][0:10])\n",
    "\n",
    "# build the vocabulary\n",
    "TEXT.build_vocab(train) # Try with random initialization\n",
    "\n",
    "# print vocab information\n",
    "print('len(TEXT.vocab)', len(TEXT.vocab))\n",
    "\n",
    "# make iterator for splits\n",
    "train_iter, valid_iter, test_iter = data.BPTTIterator.splits(\n",
    "    (train, valid, test), batch_size=64, bptt_len=100, device=\"cuda:0\")\n",
    "\n",
    "# print batch information\n",
    "batch = next(iter(train_iter))\n",
    "#print(batch.text)\n",
    "#print(batch.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qsIY3PzVLrhD"
   },
   "source": [
    "### Initialization of hyperparameters for training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GtH8-gWKazHO"
   },
   "outputs": [],
   "source": [
    "model_params = locals().copy()\n",
    "\n",
    "train_data = train_iter\n",
    "val_data = valid_iter\n",
    "test_data = test_iter\n",
    "dict_size = len(TEXT.vocab) #len(corpus.dictionary) # vocab_size \n",
    "model_params['dict_size'] = dict_size\n",
    "\n",
    "# Hyper-parameters\n",
    "num_epochs = 4\n",
    "\n",
    "#size_list = [hidden size of layer 1, hidden size of layer 2]\n",
    "size_list = [256, 256, 256] \n",
    "embed_size = 128\n",
    "model = HM_Net(1.0, size_list, dict_size, embed_size) # size_list = Number of hidden units in each layer a = 1.0\n",
    "model = model.cuda()\n",
    "\n",
    "learning_rate = 0.01\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate) \n",
    "\n",
    "# Add learning rate decay\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.8)\n",
    "\n",
    "it = 0\n",
    "start_time = time.time()\n",
    "bestPPL = 100000\n",
    "break_flag = False\n",
    "batch_size=64\n",
    "\n",
    "#indsat 25. november:\n",
    "clip=1\n",
    "\n",
    "# Track loss\n",
    "training_loss, validation_loss = [], []\n",
    "loss_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8O8f4BpFPa01"
   },
   "source": [
    "\n",
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "FQVaYZ1h69Us",
    "outputId": "b2db38b0-75af-42a6-b3a8-ac10d00e92ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model with following parameters:\n",
      "Embed dim: 128\n",
      "Hidden dim: [256, 256, 256]\n",
      "start training epoch  1 with learning rate: [0.01]\n",
      "Starting validation data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/torch/csrc/autograd/python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:62: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 Train\n",
      "200 Train\n",
      "300 Train\n",
      "400 Train\n",
      "500 Train\n",
      "600 Train\n",
      "700 Train\n",
      "--------annealing slope a to 1.2000000000000002\n",
      "Epoch:  1 finished. Time elapsed: 77.9268626968066 minutes\n",
      "training loss: 1.2489799438861378\n",
      "validation loss: 1.293416182200114\n",
      "start training epoch  2 with learning rate: [0.008]\n",
      "Starting validation data\n",
      "100 Train\n",
      "200 Train\n",
      "300 Train\n",
      "400 Train\n",
      "500 Train\n",
      "600 Train\n",
      "700 Train\n",
      "--------annealing slope a to 1.2400000000000002\n",
      "Epoch:  2 finished. Time elapsed: 93.63988607724508 minutes\n",
      "training loss: 1.2181413473939537\n",
      "validation loss: 1.2740598633175804\n",
      "start training epoch  3 with learning rate: [0.008]\n",
      "Starting validation data\n",
      "100 Train\n",
      "200 Train\n",
      "300 Train\n",
      "400 Train\n",
      "500 Train\n",
      "600 Train\n",
      "700 Train\n",
      "--------annealing slope a to 1.2800000000000002\n",
      "Epoch:  3 finished. Time elapsed: 109.36534119049708 minutes\n",
      "training loss: 1.2068926034714644\n",
      "validation loss: 1.2500217831324016\n",
      "start training epoch  4 with learning rate: [0.008]\n",
      "Starting validation data\n",
      "100 Train\n",
      "200 Train\n",
      "300 Train\n",
      "400 Train\n",
      "500 Train\n",
      "600 Train\n",
      "700 Train\n",
      "--------annealing slope a to 1.3200000000000003\n",
      "Epoch:  4 finished. Time elapsed: 125.06094532807668 minutes\n",
      "training loss: 1.2045293719248664\n",
      "validation loss: 1.2432243880771456\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJKz/B8D4lBbowqhbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Track loss\n",
    "training_loss, validation_loss = [], []\n",
    "loss_list = []\n",
    "\n",
    "print('Starting model with following parameters:')\n",
    "print('Embed dim:', embed_size)\n",
    "print('Hidden dim:', size_list)\n",
    "\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"start training epoch \", str(epoch+1), 'with learning rate:', scheduler.get_lr())\n",
    "\n",
    "    hidden_init = model.init_hidden(batch_size)\n",
    "\n",
    "    # Track loss\n",
    "    epoch_training_loss = 0\n",
    "    epoch_validation_loss = 0\n",
    "    i = 0\n",
    "    model.eval()\n",
    "        \n",
    "    # For each sentence in validation set\n",
    "    print('Starting validation data')\n",
    "    for batch in val_data:\n",
    "\n",
    "        hidden = hidden_init\n",
    "\n",
    "        inputs = batch.text\n",
    "        inputs = inputs.T\n",
    "\n",
    "        target = batch.target\n",
    "        target = target.T\n",
    "        target = target.reshape(-1)\n",
    "        \n",
    "        # Forward pass\n",
    "        loss, hidden, output, z1, z2, z3 = model(inputs, target, hidden)\n",
    "        \n",
    "        # Update loss\n",
    "        epoch_validation_loss += loss.detach().cpu().data.numpy()\n",
    "\n",
    "        #evt. hvis memory stadigvæk bliver fyldt\n",
    "        #del target, del hidden, del loss\n",
    "\n",
    "    model.train()\n",
    "    i = 0\n",
    "    for batch in train_data:\n",
    "        i += 1\n",
    "        if i%100 == 0:\n",
    "            print(i,\"Train\")\n",
    "\n",
    "        hidden = hidden_init\n",
    "        \n",
    "        optimizer.zero_grad()  # 0.0001s used\n",
    "\n",
    "        inputs = batch.text\n",
    "        inputs = inputs.T\n",
    "\n",
    "        target = batch.target\n",
    "        target = target.T\n",
    "        target = target.reshape(-1)\n",
    "               \n",
    "        loss, hidden, output, z1, z2, z3 = model(inputs, target, hidden) #tager til forward (inputs, target, hidden)\n",
    "        loss.backward()  # 3s used\n",
    "        nn.utils.clip_grad_norm(model.parameters(), clip)\n",
    "        optimizer.step()  # 0.001s used\n",
    "\n",
    "\n",
    "        # Update loss\n",
    "        epoch_training_loss += loss.detach().cpu().data.numpy()\n",
    "        \n",
    "        #evt. hvis memory stadigvæk bliver fyldt\n",
    "        #del target, del hidden, del loss\n",
    "\n",
    "    # slope annealing trick\n",
    "    scheduler.step()\n",
    "    model.HM_LSTM.cell_1.a += 0.04\n",
    "    model.HM_LSTM.cell_2.a += 0.04\n",
    "    print(\"--------annealing slope a to\", model.HM_LSTM.cell_1.a)\n",
    "    print('Epoch: ', epoch+1, 'finished. Time elapsed:', (time.time() - start_time)/60, 'minutes')\n",
    "    print('training loss:', epoch_training_loss / len(train_data))\n",
    "    print('validation loss:', epoch_validation_loss / len(val_data))\n",
    "\n",
    "    # Save loss for plot\n",
    "    training_loss.append(epoch_training_loss / len(train_data))\n",
    "    validation_loss.append(epoch_validation_loss / len(val_data))\n",
    "\n",
    "\n",
    "# Plot training and validation loss of training and validation batches\n",
    "epoch = np.arange(len(training_loss))\n",
    "plt.figure()\n",
    "plt.plot(epoch, training_loss, 'r', label='Training loss',)\n",
    "plt.plot(epoch, validation_loss, 'b', label='Validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch'), plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "TdTH_m_2hMgs",
    "outputId": "f289faee-9225-48f4-9dec-8182da720899"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/torch/csrc/autograd/python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of BPC: 1.8167461\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "# Get first sentence in test set\n",
    "loss_list = []\n",
    "BPC_list = []\n",
    "j=0\n",
    "total_loss = 0\n",
    "it = 0\n",
    "\n",
    "for batch in test_data:\n",
    "    hidden = hidden_init\n",
    "\n",
    "    inputs = batch.text.T\n",
    "    target = batch.target.T\n",
    "    target = target.reshape(-1)\n",
    "\n",
    "    loss, hidden, output, z1, z2, z3 = model.forward(inputs, target, hidden)  \n",
    "    \n",
    "    total_loss += loss.detach().cpu().data.numpy() #tester lige \n",
    "    it += 1\n",
    "\n",
    "    loss_list.append(loss.detach().cpu().data.numpy())\n",
    "\n",
    "    target_array = target.cpu().data.numpy()\n",
    "    # Bad way of doing BPC\n",
    "    #outputs_softmax = softmax(output) #BPC kræver log2 og pytorchs softmax bruger den naturlige log. Derfor kan man ikke tage mean af cross entropy\n",
    "    \n",
    "    #output_softmax_array = outputs_softmax.cpu().data.numpy()\n",
    "    \n",
    "    #summation = 0\n",
    "    #for i in range(0,len(target_array)):\n",
    "    #    prob_of_true = output_softmax_array[i][target_array[i]]\n",
    "    #    summation += np.log2(prob_of_true)\n",
    "    #BPC_list.append(- 1 / len(target_array) * summation)\n",
    "    j += 1\n",
    "    if j == 70:\n",
    "        break\n",
    "\n",
    "print('mean of BPC:', np.mean(loss_list/np.log(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "4d9RdE_2ciig",
    "outputId": "6f175fc0-83d2-4212-89fd-fdc86dbe79f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n\tn\tEMPTY\tEMPTY\tEMPTY\n",
      "g\tg\tUPDATE\tCOPY\tCOPY\n",
      "i\t \tUPDATE\tCOPY\tCOPY\n",
      "n\tn\tUPDATE\tCOPY\tCOPY\n",
      "g\tg\tUPDATE\tCOPY\tCOPY\n",
      " \t \tUPDATE\tCOPY\tCOPY\n",
      "t\tt\tUPDATE\tCOPY\tCOPY\n",
      "h\th\tUPDATE\tCOPY\tCOPY\n",
      "e\te\tUPDATE\tCOPY\tCOPY\n",
      "i\t \tUPDATE\tCOPY\tCOPY\n",
      "r\tr\tUPDATE\tCOPY\tCOPY\n",
      " \t \tUPDATE\tCOPY\tCOPY\n",
      "b\tc\tUPDATE\tCOPY\tCOPY\n",
      "r\tu\tUPDATE\tCOPY\tCOPY\n",
      "o\ti\tUPDATE\tCOPY\tCOPY\n",
      "k\tt\tUPDATE\tCOPY\tCOPY\n",
      "e\te\tUPDATE\tCOPY\tCOPY\n",
      "r\tr\tUPDATE\tCOPY\tCOPY\n",
      "s\ta\tUPDATE\tCOPY\tCOPY\n",
      " \t \tUPDATE\tCOPY\tCOPY\n",
      "w\ta\tUPDATE\tCOPY\tCOPY\n",
      "o\ti\tUPDATE\tCOPY\tCOPY\n",
      "n\tu\tUPDATE\tCOPY\tCOPY\n",
      "d\td\tUPDATE\tCOPY\tCOPY\n",
      "e\te\tUPDATE\tCOPY\tCOPY\n",
      "r\tr\tUPDATE\tCOPY\tCOPY\n",
      "i\t \tUPDATE\tCOPY\tCOPY\n",
      "n\tn\tUPDATE\tCOPY\tCOPY\n",
      "g\tg\tUPDATE\tCOPY\tCOPY\n",
      " \t \tUPDATE\tCOPY\tCOPY\n",
      "w\tt\tUPDATE\tCOPY\tCOPY\n",
      "h\ti\tUPDATE\tCOPY\tCOPY\n",
      "e\ti\tUPDATE\tCOPY\tCOPY\n",
      "t\tn\tUPDATE\tCOPY\tCOPY\n",
      "h\th\tUPDATE\tCOPY\tCOPY\n",
      "e\te\tUPDATE\tCOPY\tCOPY\n",
      "r\tr\tUPDATE\tCOPY\tCOPY\n",
      " \t \tUPDATE\tCOPY\tCOPY\n",
      "a\tt\tUPDATE\tCOPY\tCOPY\n",
      "n\t \tUPDATE\tCOPY\tCOPY\n",
      "o\t \tUPDATE\tCOPY\tCOPY\n",
      "t\tt\tUPDATE\tCOPY\tCOPY\n",
      "h\th\tUPDATE\tCOPY\tCOPY\n",
      "e\te\tUPDATE\tCOPY\tCOPY\n",
      "r\tr\tUPDATE\tCOPY\tCOPY\n",
      " \t \tUPDATE\tCOPY\tCOPY\n",
      "c\tp\tUPDATE\tCOPY\tCOPY\n",
      "r\to\tUPDATE\tCOPY\tCOPY\n",
      "a\ti\tUPDATE\tCOPY\tCOPY\n",
      "s\ts\tUPDATE\tCOPY\tCOPY\n",
      "h\th\tUPDATE\tCOPY\tCOPY\n",
      " \t \tUPDATE\tCOPY\tCOPY\n",
      "h\ta\tUPDATE\tCOPY\tCOPY\n",
      "a\ta\tUPDATE\tCOPY\tCOPY\n",
      "d\ts\tUPDATE\tCOPY\tCOPY\n",
      " \t \tUPDATE\tCOPY\tCOPY\n",
      "b\ta\tUPDATE\tCOPY\tCOPY\n",
      "e\te\tUPDATE\tCOPY\tCOPY\n",
      "g\te\tUPDATE\tCOPY\tCOPY\n",
      "u\ti\tUPDATE\tCOPY\tCOPY\n",
      "n\tn\tUPDATE\tCOPY\tCOPY\n",
      " \t \tUPDATE\tCOPY\tCOPY\n",
      "<eos>\ta\tUPDATE\tCOPY\tCOPY\n",
      " \t \tUPDATE\tCOPY\tCOPY\n",
      "a\tt\tUPDATE\tCOPY\tCOPY\n",
      "t\t \tUPDATE\tCOPY\tCOPY\n",
      " \t \tUPDATE\tCOPY\tCOPY\n",
      "p\tt\tUPDATE\tCOPY\tCOPY\n",
      "r\tr\tUPDATE\tCOPY\tCOPY\n",
      "u\to\tUPDATE\tCOPY\tCOPY\n",
      "d\td\tUPDATE\tCOPY\tCOPY\n",
      "e\te\tUPDATE\tCOPY\tCOPY\n",
      "n\tn\tUPDATE\tCOPY\tCOPY\n",
      "t\tt\tUPDATE\tCOPY\tCOPY\n",
      "i\t \tUPDATE\tCOPY\tCOPY\n",
      "a\ta\tUPDATE\tCOPY\tCOPY\n",
      "l\tl\tUPDATE\tCOPY\tCOPY\n",
      "-\t \tUPDATE\tCOPY\tCOPY\n",
      "b\ts\tUPDATE\tCOPY\tCOPY\n",
      "a\ta\tUPDATE\tCOPY\tCOPY\n",
      "c\tc\tUPDATE\tCOPY\tCOPY\n",
      "h\th\tUPDATE\tCOPY\tCOPY\n",
      "e\te\tUPDATE\tCOPY\tCOPY\n",
      " \t \tUPDATE\tCOPY\tCOPY\n",
      "s\ta\tUPDATE\tCOPY\tCOPY\n",
      "e\ta\tUPDATE\tCOPY\tCOPY\n",
      "c\tc\tUPDATE\tCOPY\tCOPY\n",
      "u\tu\tUPDATE\tCOPY\tCOPY\n",
      "r\tr\tUPDATE\tCOPY\tCOPY\n",
      "i\ti\tUPDATE\tCOPY\tCOPY\n",
      "t\tt\tUPDATE\tCOPY\tCOPY\n",
      "i\ti\tUPDATE\tCOPY\tCOPY\n",
      "e\te\tUPDATE\tCOPY\tCOPY\n",
      "s\ts\tUPDATE\tCOPY\tCOPY\n",
      " \t \tUPDATE\tCOPY\tCOPY\n",
      "i\ta\tUPDATE\tCOPY\tCOPY\n",
      "n\tn\tUPDATE\tCOPY\tCOPY\n",
      "c\t \tUPDATE\tCOPY\tCOPY\n",
      ".\t.\tUPDATE\tCOPY\tCOPY\n",
      " \t \tUPDATE\tCOPY\tCOPY\n",
      "r\t \tUPDATE\tCOPY\tCOPY\n",
      " \t \tUPDATE\tCOPY\tCOPY\n",
      "t\tt\tUPDATE\tCOPY\tCOPY\n",
      "h\th\tUPDATE\tCOPY\tCOPY\n",
      "e\te\tUPDATE\tCOPY\tCOPY\n",
      " \t \tUPDATE\tCOPY\tCOPY\n",
      "d\tc\tUPDATE\tCOPY\tCOPY\n",
      "i\te\tUPDATE\tCOPY\tCOPY\n",
      "r\ts\tUPDATE\tCOPY\tCOPY\n",
      "e\te\tUPDATE\tCOPY\tCOPY\n",
      "c\tc\tUPDATE\tCOPY\tCOPY\n",
      "t\tt\tUPDATE\tCOPY\tCOPY\n",
      "i\to\tUPDATE\tCOPY\tCOPY\n",
      "o\to\tUPDATE\tCOPY\tCOPY\n",
      "n\tn\tUPDATE\tCOPY\tCOPY\n",
      " \t \tUPDATE\tCOPY\tCOPY\n",
      "o\to\tUPDATE\tCOPY\tCOPY\n",
      "f\tf\tUPDATE\tCOPY\tCOPY\n",
      " \t \tUPDATE\tCOPY\tCOPY\n",
      "t\tt\tUPDATE\tCOPY\tCOPY\n",
      "h\th\tUPDATE\tCOPY\tCOPY\n",
      "e\te\tUPDATE\tCOPY\tCOPY\n",
      " \t \tUPDATE\tCOPY\tCOPY\n",
      "c\tc\tUPDATE\tCOPY\tCOPY\n",
      "o\to\tUPDATE\tCOPY\tCOPY\n",
      "m\tm\tUPDATE\tCOPY\tCOPY\n",
      "p\tp\tUPDATE\tCOPY\tCOPY\n",
      "a\ta\tUPDATE\tCOPY\tCOPY\n",
      "n\tn\tUPDATE\tCOPY\tCOPY\n",
      "y\ty\tUPDATE\tCOPY\tCOPY\n",
      " \t \tUPDATE\tCOPY\tCOPY\n",
      "a\ta\tUPDATE\tCOPY\tCOPY\n",
      "n\ts\tUPDATE\tCOPY\tCOPY\n",
      "d\td\tUPDATE\tCOPY\tCOPY\n",
      " \t \tUPDATE\tCOPY\tCOPY\n",
      "m\tt\tUPDATE\tCOPY\tCOPY\n",
      "r\ta\tUPDATE\tCOPY\tCOPY\n",
      ".\t.\tUPDATE\tCOPY\tCOPY\n",
      " \t \tUPDATE\tCOPY\tCOPY\n",
      "s\t<\tUPDATE\tCOPY\tCOPY\n",
      "i\ta\tUPDATE\tCOPY\tCOPY\n",
      "m\tm\tUPDATE\tCOPY\tCOPY\n",
      "p\ti\tUPDATE\tCOPY\tCOPY\n",
      "s\tl\tUPDATE\tCOPY\tCOPY\n",
      "o\to\tUPDATE\tCOPY\tCOPY\n",
      "n\tn\tUPDATE\tCOPY\tCOPY\n",
      " \t \tUPDATE\tCOPY\tCOPY\n",
      "s\ta\tUPDATE\tCOPY\tCOPY\n",
      "a\ta\tUPDATE\tCOPY\tCOPY\n",
      "i\tl\tUPDATE\tCOPY\tCOPY\n",
      "d\td\tUPDATE\tCOPY\tCOPY\n",
      " \t \tUPDATE\tCOPY\tCOPY\n",
      "h\tt\tUPDATE\tCOPY\tCOPY\n",
      "e\te\tUPDATE\tCOPY\tCOPY\n",
      " \t \tUPDATE\tCOPY\tCOPY\n",
      "r\ts\tUPDATE\tCOPY\tCOPY\n",
      "e\te\tUPDATE\tCOPY\tCOPY\n",
      "s\tp\tUPDATE\tCOPY\tCOPY\n",
      "i\te\tUPDATE\tCOPY\tCOPY\n",
      "g\ts\tUPDATE\tCOPY\tCOPY\n",
      "n\tn\tUPDATE\tCOPY\tCOPY\n",
      "e\te\tUPDATE\tCOPY\tCOPY\n",
      "d\td\tUPDATE\tCOPY\tCOPY\n",
      " \t \tUPDATE\tCOPY\tCOPY\n",
      "i\tt\tUPDATE\tCOPY\tCOPY\n",
      "n\tn\tUPDATE\tCOPY\tCOPY\n",
      " \t \tUPDATE\tCOPY\tCOPY\n",
      "n\tt\tUPDATE\tCOPY\tCOPY\n",
      " \t \tUPDATE\tCOPY\tCOPY\n",
      "<eos>\t<eos>\tUPDATE\tCOPY\tCOPY\n",
      " \t \tUPDATE\tCOPY\tCOPY\n",
      "s\tt\tUPDATE\tCOPY\tCOPY\n",
      "i\th\tUPDATE\tCOPY\tCOPY\n",
      "n\tn\tUPDATE\tCOPY\tCOPY\n",
      "c\tc\tUPDATE\tCOPY\tCOPY\n",
      "e\te\tUPDATE\tCOPY\tCOPY\n",
      " \t \tUPDATE\tCOPY\tCOPY\n",
      "t\tt\tUPDATE\tCOPY\tCOPY\n",
      "h\th\tUPDATE\tCOPY\tCOPY\n",
      "e\te\tUPDATE\tCOPY\tCOPY\n",
      "n\t \tUPDATE\tCOPY\tCOPY\n",
      " \t \tUPDATE\tCOPY\tCOPY\n",
      "h\ta\tUPDATE\tCOPY\tCOPY\n",
      "o\ta\tUPDATE\tCOPY\tCOPY\n",
      "o\tu\tUPDATE\tCOPY\tCOPY\n",
      "k\tl\tUPDATE\tCOPY\tCOPY\n",
      "e\te\tUPDATE\tCOPY\tCOPY\n",
      "r\ts\tUPDATE\tCOPY\tCOPY\n",
      " \t \tUPDATE\tCOPY\tCOPY\n",
      "c\tt\tUPDATE\tCOPY\tCOPY\n",
      "o\to\tUPDATE\tCOPY\tCOPY\n",
      "r\tm\tUPDATE\tCOPY\tCOPY\n",
      "p\tp\tUPDATE\tCOPY\tCOPY\n",
      ".\t.\tUPDATE\tCOPY\tCOPY\n",
      " \t \tUPDATE\tCOPY\tCOPY\n",
      "h\ta\tUPDATE\tCOPY\tCOPY\n",
      "a\ta\tUPDATE\tCOPY\tCOPY\n",
      "s\ts\tUPDATE\tCOPY\tCOPY\n",
      " \t \tUPDATE\tCOPY\tCOPY\n",
      "s\ta\tUPDATE\tCOPY\tCOPY\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "z1_array = z1.view(-1, 100*64)\n",
    "z1_array = z1_array.cpu().data.numpy()\n",
    "\n",
    "z2_array = z2.view(-1, 64*100)\n",
    "z2_array = z2_array.cpu().data.numpy()\n",
    "\n",
    "z3_array = z3.view(-1, 64*100)\n",
    "z3_array = z3_array.cpu().data.numpy()\n",
    "\n",
    "l1_state, l2_state, l3_state = 'EMPTY', 'EMPTY', 'EMPTY'\n",
    "\n",
    "output_array = output.cpu().data.numpy()\n",
    "for i in range(0,200,1):\n",
    "    #predict\n",
    "    char_vector = output_array[i]\n",
    "    char = np.argmax(char_vector)\n",
    "    predict_char = TEXT.vocab.itos[char]\n",
    "\n",
    "    # True\n",
    "    char = target_array[i]\n",
    "    true_char = TEXT.vocab.itos[char]\n",
    "\n",
    "    if i > 0:\n",
    "      # layer 1\n",
    "      if z1_array[0, i-1] == 1:\n",
    "        l1_state = 'FLUSH' # Flush\n",
    "      else:\n",
    "        l1_state = 'UPDATE' # update\n",
    "\n",
    "      # Layer 2\n",
    "      if z2_array[0, i-1] == 1:\n",
    "        l2_state = 'FLUSH' # Flush\n",
    "      elif z2_array[0, i-1] == 0 and z1_array[0, i] == 0:\n",
    "        l2_state = 'COPY' # Copy\n",
    "      elif z2_array[0, i-1] == 0 and z1_array[0, i] == 1:\n",
    "        l2_state = 'UPDATE' # update\n",
    "      \n",
    "      # Layer 3\n",
    "      if z3_array[0, i-1] == 1:\n",
    "        l3_state = 'FLUSH' # Flush\n",
    "      elif z3_array[0, i-1] == 0 and z2_array[0, i] == 0:\n",
    "        l3_state = 'COPY' # Copy\n",
    "      elif z3_array[0, i-1] == 0 and z2_array[0, i] == 1:\n",
    "        l3_state = 'UPDATE' # update\n",
    "    \n",
    "    print(true_char, predict_char, l1_state, l2_state, l3_state, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "6SNQEOLKln2B",
    "outputId": "f9a20f0d-ae69-41cc-b62a-ac14f0e2a8de"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/torch/csrc/autograd/python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n"
     ]
    }
   ],
   "source": [
    "# Generation of text\n",
    "\n",
    "# Convert text to numeric input\n",
    "input_text = 'new'\n",
    "input_text = list(input_text)\n",
    "\n",
    "text_numeric = []\n",
    "\n",
    "for char in input_text:\n",
    "  text_numeric.append(TEXT.vocab.stoi[char])\n",
    "\n",
    "\n",
    "# Run through model\n",
    "num_generations = 100\n",
    "input_array = np.array(text_numeric)\n",
    "\n",
    "hidden_init = model.init_hidden(1)\n",
    "hidden = hidden_init\n",
    "\n",
    "exp_dim = len(input_array)\n",
    "\n",
    "input_tensor = torch.from_numpy(input_array)\n",
    "input_tensor = input_tensor.expand(1,exp_dim)\n",
    "input_tensor = input_tensor.view(exp_dim,-1)\n",
    "input_tensor = input_tensor.T\n",
    "input_tensor = input_tensor.cuda()\n",
    "\n",
    "target_tensor = torch.ones(exp_dim).long().cuda()\n",
    "loss, hidden, outputs, z1, z2, z3 = model.forward(input_tensor,target_tensor, hidden)\n",
    "\n",
    "values, indices = outputs[-1].max(0)\n",
    "\n",
    "input_array = np.append(input_array, indices.cpu().numpy())\n",
    "\n",
    "for i in range(num_generations):\n",
    "  hidden_init = model.init_hidden(1)\n",
    "  hidden = hidden_init\n",
    "  exp_dim = len(input_array)\n",
    "  input_tensor = torch.from_numpy(input_array)\n",
    "  input_tensor = input_tensor.expand(1,exp_dim)\n",
    "  input_tensor = input_tensor.view(exp_dim,-1)\n",
    "  input_tensor = input_tensor.T\n",
    "  input_tensor = input_tensor.cuda()\n",
    "  target_tensor = torch.ones(exp_dim).long().cuda()\n",
    "  loss, hidden, outputs, z1, z2, z3 = model.forward(input_tensor,target_tensor, hidden)\n",
    "\n",
    "  values, indices = outputs[-1].max(0)\n",
    "\n",
    "  input_array = np.append(input_array, indices.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xV3N4kIKo722",
    "outputId": "ae7470ee-29be-4a7a-c2fa-02885c267485"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new a <unk> and the company as a <unk> and the company as a <unk> and the company as a <unk> and the com\n"
     ]
    }
   ],
   "source": [
    "predicted_sentence = list() \n",
    "for char in input_array:\n",
    "  predict_char = TEXT.vocab.itos[char]\n",
    "  predicted_sentence.append(predict_char)\n",
    "\n",
    "print(''.join(predicted_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hQPW31yirQYx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Kopi af Gustav_3L.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
