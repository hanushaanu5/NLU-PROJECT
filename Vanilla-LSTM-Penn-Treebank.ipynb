{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qye2DOBijVby"
   },
   "source": [
    "# Vanilla LSTM model with Penn Treebank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zUb41UsnHdwy"
   },
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "colab_type": "code",
    "id": "l8jwdmfpxk8D",
    "outputId": "9979144e-9752-472f-af88-3d0f20f75776"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.functional import softmax\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.optim as optim\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "guEVtuTCjlmt"
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "3JBQTXPji5b1",
    "outputId": "612dfabf-1e2d-4dbd-e28d-b2a74069db10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.fields {'text': <torchtext.data.field.Field object at 0x7f78fe99e1d0>}\n",
      "len(train) 1\n",
      "vars(train[0]) [' ', 'a', 'e', 'r', ' ', 'b', 'a', 'n', 'k', 'n']\n",
      "len(TEXT.vocab) 51\n"
     ]
    }
   ],
   "source": [
    "#Tokenizer: splits sentences into char tokens\n",
    "def tokenize(lines):\n",
    "    return [line for line in lines]\n",
    "\n",
    "# set up fields\n",
    "TEXT = data.Field(lower=True, tokenize=tokenize, batch_first=True)\n",
    "\n",
    "# make splits for data\n",
    "train, valid, test = datasets.PennTreebank.splits(TEXT)\n",
    "\n",
    "# print information about the data\n",
    "print('train.fields', train.fields)\n",
    "print('len(train)', len(train))\n",
    "print('vars(train[0])', vars(train[0])['text'][0:10])\n",
    "\n",
    "# build the vocabulary\n",
    "TEXT.build_vocab(train)\n",
    "\n",
    "# print vocab information\n",
    "print('len(TEXT.vocab)', len(TEXT.vocab))\n",
    "\n",
    "# make iterator for splits\n",
    "train_iter, valid_iter, test_iter = data.BPTTIterator.splits(\n",
    "    (train, valid, test), batch_size=64, bptt_len=100)\n",
    "\n",
    "# print batch information\n",
    "batch = next(iter(train_iter))\n",
    "#print(batch.text)\n",
    "#print(batch.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "99AqEr8AjoRu"
   },
   "source": [
    "### Activate cuda "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9ux1Q0es0dnJ",
    "outputId": "2fd6ba88-a458-4dd6-d94a-bba8121fe7cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GPU.\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"Running GPU.\") if use_cuda else print(\"No GPU available.\")\n",
    "\n",
    "def get_variable(x):\n",
    "    \"\"\" Converts tensors to cuda, if available. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cuda()\n",
    "    return x\n",
    "\n",
    "def get_numpy(x):\n",
    "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cpu().data.numpy()\n",
    "    return x.data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tV4bWiJnHYQW"
   },
   "source": [
    "### Define network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "mR3S8ra2GgwN",
    "outputId": "0b36df82-aa26-4c07-d99d-35699711f0be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (lstm): LSTM(128, 1024)\n",
      "  (l_out): Linear(in_features=1024, out_features=51, bias=False)\n",
      "  (embed): Embedding(51, 128)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Set hyperparameters\n",
    "hidden_dim = 1024\n",
    "vocab_size = len(TEXT.vocab)\n",
    "emb_dim = 128\n",
    "num_epochs = 5\n",
    "learning_rate = 0.01\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Recurrent layer\n",
    "        self.lstm = nn.LSTM(input_size = emb_dim, hidden_size = hidden_dim)\n",
    "        \n",
    "        # Output layer\n",
    "        self.l_out = nn.Linear(in_features=hidden_dim, out_features=51, bias=False)\n",
    "      \n",
    "        # embed\n",
    "        self.embed = nn.Embedding(num_embeddings = len(TEXT.vocab), embedding_dim = emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.cuda()\n",
    "        x = self.embed(x)\n",
    "  \n",
    "        # RNN returns output and last hidden state\n",
    "        x, (h, c) = self.lstm(x)\n",
    "\n",
    "        # Flatten output for feed-forward layer\n",
    "        x = x.view(-1, hidden_dim)\n",
    "\n",
    "        # Output layer\n",
    "        x = self.l_out(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Initialize a new network\n",
    "net = Net()\n",
    "net.cuda()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4WpALf2-x7Ty"
   },
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "colab_type": "code",
    "id": "2URKsyFDx8xG",
    "outputId": "52e69471-850e-44c7-8b1a-50500fe48b81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 LR: [0.01]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training loss: 0.0002009437421083159, validation loss: 0.0006195691795375266\n",
      "Epoch: 1 LR: [0.01]\n",
      "Epoch 1, training loss: 0.00018000329015651286, validation loss: 0.00018952757126245253\n",
      "Epoch: 2 LR: [0.01]\n",
      "Epoch 2, training loss: 0.000184389497700235, validation loss: 0.00018893811740311602\n",
      "Epoch: 3 LR: [0.0085]\n",
      "Epoch 3, training loss: 0.00018287062297865715, validation loss: 0.00019662402469125412\n",
      "Epoch: 4 LR: [0.0085]\n",
      "Epoch 4, training loss: 0.00018581301247694534, validation loss: 0.0001880122217716062\n"
     ]
    }
   ],
   "source": [
    "# Loss function and optimizer \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr = learning_rate)\n",
    "\n",
    "# Add learning rate decay\n",
    "scheduler = StepLR(optimizer, step_size=4, gamma=0.85)\n",
    "\n",
    "# Track loss\n",
    "training_loss, validation_loss = [], []\n",
    "loss_list = []\n",
    "\n",
    "# For each epoch\n",
    "for i in range(num_epochs):\n",
    "    # Decay learning rate\n",
    "    scheduler.step()\n",
    "    print('Epoch:', i,'LR:', scheduler.get_lr())\n",
    "\n",
    "    # Track loss\n",
    "    epoch_training_loss = 0\n",
    "    epoch_validation_loss = 0\n",
    "    \n",
    "    net.eval()\n",
    "        \n",
    "    # For each sentence in validation set\n",
    "    for batch in valid_iter:\n",
    "        \n",
    "        text = batch.text\n",
    "        target = batch.target\n",
    "        target = target.cuda()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = net(text)\n",
    "        \n",
    "        # Compute loss\n",
    "        target = target.view(-1, target.shape[0]*target.shape[1])[0] \n",
    "        loss = criterion(outputs,target)\n",
    "        \n",
    "        # Update loss\n",
    "        epoch_validation_loss += loss.detach().cpu().data.numpy()\n",
    "    \n",
    "    net.train()\n",
    "    \n",
    "    # For each sentence in training set\n",
    "    for batch in train_iter:\n",
    "\n",
    "        text = batch.text\n",
    "        target = batch.target\n",
    "        target = target.cuda()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = net(text)\n",
    "        \n",
    "        # Compute loss\n",
    "        target = target.view(-1, target.shape[0]*target.shape[1])[0]\n",
    "        loss = criterion(outputs,target) \n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "              \n",
    "        # Update loss\n",
    "        epoch_training_loss += loss.detach().cpu().data.numpy()\n",
    "        \n",
    "    # Save loss for plot\n",
    "    training_loss.append(epoch_training_loss / len(vars(train[0])['text'])) \n",
    "    validation_loss.append(epoch_validation_loss / len(vars(valid[0])['text']))\n",
    "\n",
    "    # Print loss every\n",
    "    print(f'Epoch {i}, training loss: {training_loss[-1]}, validation loss: {validation_loss[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PCXanSJlnlo3"
   },
   "source": [
    "### Test loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "colab_type": "code",
    "id": "2GGmH7U6JmaG",
    "outputId": "f22cdfd7-2eed-45b7-8a73-7609a6dbf911"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEGCAYAAABCa2PoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of BPC: 1.8758946034644368\n"
     ]
    }
   ],
   "source": [
    "# Get first sentence in test set\n",
    "loss_list = []\n",
    "BPC_list = []\n",
    "j=0\n",
    "\n",
    "for batch in test_iter:\n",
    "    text, target = batch.text.cuda(), batch.target.cuda()\n",
    "    outputs = net.forward(text)\n",
    "    target = target.view(-1, target.shape[0]*target.shape[1])[0]\n",
    "\n",
    "    #Update loss\n",
    "    loss = criterion(outputs, target)\n",
    "    loss_list.append(loss)\n",
    "\n",
    "    outputs_softmax = softmax(outputs)\n",
    "\n",
    "    output_softmax_array = outputs_softmax.cpu().data.numpy()\n",
    "    target_array = target.cpu().data.numpy()\n",
    "    summation = 0\n",
    "    for i in range(0,len(target_array)):\n",
    "      prob_of_true = output_softmax_array[i][target_array[i]]\n",
    "      summation += np.log2(prob_of_true)\n",
    "    BPC_list.append(- 1 / len(target_array) * summation)\n",
    "    j += 1\n",
    "    if j == 50:\n",
    "      break\n",
    "\n",
    "    #To avoid running out of memeory\n",
    "    del target\n",
    "    del loss\n",
    "    del outputs\n",
    "\n",
    "# Plot training and validation loss\n",
    "epoch = np.arange(len(training_loss))\n",
    "plt.figure()\n",
    "plt.plot(epoch, training_loss, 'r', label='Training loss',)\n",
    "plt.plot(epoch, validation_loss, 'b', label='Validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch'), plt.ylabel('NLL')\n",
    "plt.show()\n",
    "\n",
    "print('mean of BPC:', np.mean(BPC_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Da8BBf2Knnvf"
   },
   "source": [
    "### Predicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "FlzXJgZhRXex",
    "outputId": "0a0dc86c-6a50-4a1c-fee4-c013650050c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s\t \n",
      " \t \n",
      "u\t<eos>\n",
      "p\tn\n",
      " \t \n",
      "s\tt\n",
      "a\ta\n",
      "i\ti\n",
      "d\td\n",
      " \t \n",
      "o\tt\n",
      "n\tn\n",
      "e\t \n",
      " \t \n",
      "m\tt\n",
      "a\ta\n",
      "j\tr\n",
      "o\to\n",
      "r\tr\n",
      " \t \n",
      "s\ta\n",
      "p\tt\n",
      "e\te\n",
      "c\tn\n",
      "i\ti\n",
      "a\tf\n",
      "l\tl\n",
      "i\t \n",
      "s\ts\n",
      "t\tt\n",
      " \ts\n",
      "<eos>\t<eos>\n",
      " \t \n",
      "t\tt\n",
      "h\th\n",
      "i\te\n",
      "s\ts\n",
      " \t \n",
      "c\tw\n",
      "o\to\n",
      "n\tm\n",
      "f\tt\n",
      "u\te\n",
      "s\ts\n",
      "i\ti\n",
      "o\tt\n",
      "n\tn\n",
      " \t \n",
      "e\tt\n",
      "f\tx\n",
      "f\tf\n",
      "e\te\n",
      "c\tc\n",
      "t\tt\n",
      "i\t \n",
      "v\tv\n",
      "e\te\n",
      "l\t \n",
      "y\ty\n",
      " \t \n",
      "h\ta\n",
      "a\ta\n",
      "l\td\n",
      "t\tf\n",
      "e\t \n",
      "d\tr\n",
      " \t \n",
      "o\ta\n",
      "n\tn\n",
      "e\t \n",
      " \t \n",
      "f\tt\n",
      "o\to\n",
      "r\tr\n",
      "m\t \n",
      " \te\n",
      "o\tt\n",
      "f\tf\n",
      " \t \n",
      "p\tt\n",
      "r\tr\n",
      "o\te\n",
      "g\tf\n",
      "r\tr\n",
      "a\ta\n",
      "m\tm\n",
      " \t \n",
      "t\tt\n",
      "r\tr\n",
      "a\ta\n",
      "d\td\n",
      "i\ti\n",
      "n\tn\n",
      "g\tg\n",
      " \t \n",
      "s\tt\n",
      "t\ta\n",
      "o\to\n",
      "c\tc\n",
      "k\tk\n",
      " \t \n",
      "a\tt\n",
      "s\tn\n",
      " \t \n",
      "r\tt\n",
      "e\te\n",
      "p\tl\n",
      "r\to\n",
      "e\te\n",
      "s\ts\n",
      "e\ti\n",
      "n\tn\n",
      "t\tt\n",
      "i\ta\n",
      "n\ta\n",
      "g\tg\n",
      " \t \n",
      "n\tt\n",
      " \to\n",
      "n\tn\n",
      " \t \n",
      "o\tt\n",
      "f\tf\n",
      " \t \n",
      "t\tt\n",
      "h\th\n",
      "e\te\n",
      " \t \n",
      "v\ts\n",
      "a\te\n",
      "l\tl\n",
      "u\tu\n",
      "e\te\n",
      " \t \n",
      "o\to\n",
      "f\tf\n",
      " \t \n",
      "a\tt\n",
      "l\t \n",
      "l\tl\n",
      " \t \n",
      "h\tt\n",
      "o\te\n",
      "o\tu\n",
      "k\tk\n",
      "e\tl\n",
      "r\tr\n",
      " \ts\n",
      "r\t<eos>\n",
      "e\te\n",
      "a\ts\n",
      "l\tl\n",
      "-\t \n",
      "e\tb\n",
      "s\ta\n",
      "t\ts\n",
      "a\ti\n",
      "t\tt\n",
      "e\te\n",
      " \t \n",
      "h\ta\n",
      "o\ta\n",
      "l\tu\n",
      "d\td\n",
      "i\ti\n",
      "n\tn\n",
      "g\tg\n",
      "s\t \n",
      " \t \n",
      "i\t<eos>\n",
      "n\tn\n",
      " \t \n",
      "t\tt\n",
      "h\th\n",
      "e\te\n",
      " \t \n",
      "u\ts\n",
      ".\t.\n",
      "s\ts\n",
      ".\t.\n",
      " \t \n",
      "<eos>\tm\n",
      " \t \n",
      "n\tt\n",
      "o\te\n",
      "t\tt\n",
      " \t \n",
      "i\tt\n",
      "n\tn\n",
      "c\t \n",
      "l\t.\n",
      "u\tu\n",
      "d\td\n",
      "e\ti\n",
      "d\td\n",
      " \t \n",
      "i\tt\n",
      "n\tt\n",
      " \t \n",
      "t\tt\n",
      ">\t>\n",
      " \t \n",
      "a\ta\n",
      "t\tn\n",
      " \t \n",
      "t\tt\n",
      "w\th\n",
      "i\to\n",
      "c\tn\n",
      "e\t \n",
      " \t \n",
      "t\t<eos>\n",
      "h\th\n",
      "e\te\n",
      " \t \n",
      "n\ts\n",
      "o\t \n",
      "r\tt\n",
      "m\tt\n",
      "a\ta\n",
      "l\tl\n",
      " \t \n",
      "w\tb\n",
      "e\ta\n",
      "e\te\n",
      "k\tk\n",
      "e\t \n",
      "n\tn\n",
      "d\t \n",
      " \t \n",
      "p\tt\n",
      "a\tr\n",
      "c\tc\n",
      "e\tk\n",
      " \t \n",
      "y\ta\n",
      "e\te\n",
      "s\ts\n",
      "t\tt\n",
      "e\te\n",
      "r\tr\n",
      "d\td\n",
      "a\ta\n",
      "y\ty\n",
      " \t \n",
      "<eos>\t<eos>\n",
      " \t \n",
      "b\tt\n",
      "u\tu\n",
      "t\tt\n",
      " \t \n",
      "m\tt\n",
      "o\ta\n",
      "s\tr\n",
      "t\tt\n",
      " \t \n",
      "i\ts\n",
      "n\tn\n",
      "v\t \n",
      "e\te\n",
      "s\ts\n",
      "t\tt\n",
      "o\to\n",
      "r\tr\n",
      "s\ts\n",
      " \t \n",
      "w\ta\n",
      "e\th\n",
      "r\tr\n",
      "e\te\n",
      " \t \n",
      "s\tn\n",
      "e\tt\n",
      "e\tr\n",
      "k\tk\n",
      "i\ti\n",
      "n\tn\n",
      "g\tg\n",
      " \t \n",
      "s\tt\n",
      "h\tt\n",
      "a\ta\n",
      "r\tr\n",
      "e\te\n",
      " \ts\n",
      "p\ta\n",
      "r\tr\n",
      "i\te\n",
      "c\tc\n",
      "e\te\n",
      "s\t \n",
      " \t \n",
      "a\t<eos>\n",
      "n\tn\n",
      "d\td\n",
      " \t \n",
      "o\tt\n",
      "t\tt\n",
      "h\th\n",
      "e\te\n",
      "i\t \n",
      "n\tt\n",
      "g\tg\n",
      " \t \n",
      "o\tt\n",
      "n\tn\n",
      " \t \n",
      "t\tt\n",
      "h\th\n",
      "e\te\n",
      " \t \n",
      "n\t<\n",
      "e\te\n",
      "w\tw\n",
      " \t \n",
      "y\ty\n",
      "o\to\n",
      "r\tr\n",
      "k\tk\n",
      " \t \n",
      "s\ts\n",
      "t\tt\n",
      "o\ta\n",
      "c\tc\n",
      "k\tk\n",
      " \t \n",
      "e\t<eos>\n",
      "x\tx\n",
      "c\tp\n",
      "h\th\n",
      "a\ta\n",
      "n\tn\n",
      "g\tg\n",
      "e\te\n",
      " \t \n",
      "<eos>\t<eos>\n",
      " \t \n",
      "b\tt\n",
      "u\tu\n",
      "t\tt\n",
      " \t \n",
      "m\tt\n",
      "r\ta\n",
      ".\t.\n",
      " \t \n",
      "b\t<\n",
      "a\tr\n",
      "k\tn\n",
      "e\te\n",
      "r\tr\n",
      " \t \n",
      "s\tt\n",
      "a\ta\n",
      "i\ti\n",
      "d\td\n",
      " \t \n",
      "h\tt\n",
      "e\te\n",
      " \t \n",
      "t\ts\n",
      "h\th\n",
      "i\te\n",
      "n\tn\n",
      "k\tk\n",
      "s\t \n",
      " \t \n",
      "t\ta\n",
      "h\th\n",
      "e\te\n",
      " \t \n",
      "e\ts\n",
      "a\tn\n",
      "r\tr\n",
      "l\tl\n",
      "i\ti\n",
      "e\te\n",
      "s\tr\n",
      "t\t \n",
      " \t \n",
      "a\tc\n",
      " \tn\n",
      "p\t<\n",
      "a\tr\n",
      "c\tr\n",
      "t\tk\n",
      " \t \n",
      "c\tt\n",
      "o\to\n",
      "u\tm\n",
      "l\tn\n",
      "d\td\n",
      " \t \n",
      "b\tb\n",
      "e\te\n",
      " \t \n",
      "s\ts\n",
      "t\te\n",
      "r\tr\n",
      "u\te\n",
      "c\tc\n",
      "n\tr\n",
      "e\t \n",
      " \t \n",
      "o\t<eos>\n",
      "f\tf\n",
      " \t \n",
      "t\tt\n",
      "h\th\n",
      "e\te\n",
      " \t \n",
      "b\t<\n",
      "e\ta\n",
      "s\t \n",
      "t\tt\n",
      " \t \n",
      "w\ta\n",
      "a\te\n",
      "y\ts\n",
      "s\t \n",
      " \t \n",
      "t\tt\n",
      "o\to\n",
      " \t \n",
      "a\t<\n",
      "s\tc\n",
      "s\ts\n",
      "u\te\n",
      "r\tm\n",
      "e\te\n",
      " \t \n",
      "a\tt\n",
      " \t \n",
      "p\t<\n",
      "i\tr\n",
      "p\tc\n",
      "e\te\n",
      "l\ts\n",
      "i\ti\n",
      "n\tn\n",
      "e\te\n",
      " \t \n",
      "o\t<eos>\n",
      "f\tf\n",
      " \t \n",
      "l\tt\n",
      "a\te\n",
      "n\tr\n",
      "d\tg\n",
      " \ta\n",
      "t\tt\n",
      "o\th\n",
      " \t \n",
      "f\t<\n",
      "u\ta\n",
      "e\tn\n",
      "l\tl\n",
      " \t \n",
      "o\tt\n",
      "u\tn\n",
      "r\tt\n",
      " \t \n",
      "g\t<\n",
      "r\to\n",
      "o\to\n",
      "w\tu\n",
      "t\tt\n",
      "h\th\n",
      " \t \n",
      "a\ta\n",
      "t\tn\n",
      " \t \n",
      "a\tt\n",
      " \t \n",
      "m\t<\n",
      "i\te\n",
      "n\tn\n",
      "i\ti\n",
      "m\ts\n",
      "u\tu\n",
      "m\tm\n",
      " \t \n",
      "r\t<eos>\n",
      "i\te\n",
      "s\ts\n",
      "k\tk\n",
      " \t \n",
      "t\t<eos>\n",
      "o\to\n",
      " \t \n",
      "o\tb\n",
      "u\tp\n",
      "r\tt\n",
      " \t \n",
      "c\t<\n",
      "o\to\n",
      "m\tn\n",
      "p\tp\n",
      "a\ta\n",
      "n\tn\n",
      "y\ti\n",
      "h\t \n",
      "e\te\n",
      " \t \n",
      "c\tc\n",
      "o\to\n",
      "m\tm\n",
      "p\tp\n",
      "a\ta\n",
      "n\tn\n",
      "y\ty\n",
      " \t \n",
      "'\t'\n",
      "s\ts\n",
      " \t \n",
      "r\ts\n",
      "a\te\n",
      "w\ti\n",
      " \t \n",
      "m\tt\n",
      "a\ta\n",
      "t\tr\n",
      "e\tt\n",
      "r\tr\n",
      "i\ti\n",
      "a\ta\n",
      "l\tl\n",
      " \t \n",
      "c\ta\n",
      "o\to\n",
      "s\tn\n",
      "t\tt\n",
      "s\ts\n",
      " \t \n",
      "<eos>\ta\n",
      " \t \n",
      "q\tt\n",
      "u\tu\n",
      "a\te\n",
      "n\tr\n",
      "t\tt\n",
      "u\ti\n",
      "m\tr\n",
      " \te\n",
      "i\ta\n",
      "s\tn\n",
      " \t \n",
      "a\tn\n",
      "l\t \n",
      "s\ts\n",
      "o\to\n",
      " \t \n",
      "t\ta\n",
      "i\to\n",
      "g\te\n",
      "h\th\n",
      "t\tt\n",
      "e\te\n",
      "n\td\n",
      "i\td\n",
      "n\tn\n",
      "g\tg\n",
      " \t \n",
      "i\tt\n",
      "t\tn\n",
      "s\ts\n",
      " \t \n",
      "g\tc\n",
      "r\tr\n",
      "i\to\n",
      "p\td\n",
      " \t \n",
      "o\t<eos>\n",
      "n\tf\n",
      " \t \n",
      "i\tt\n",
      "t\tt\n",
      "s\ts\n",
      " \t \n",
      "o\tt\n",
      "n\tf\n",
      "e\t \n",
      " \t \n",
      "l\to\n",
      "a\te\n",
      "r\ts\n",
      "g\tg\n",
      "e\te\n",
      " \ts\n",
      "b\t<eos>\n",
      "u\ta\n",
      "s\tt\n",
      "i\ti\n",
      "n\tn\n",
      "e\te\n",
      "s\ts\n",
      "s\ts\n",
      " \t \n",
      "o\t<eos>\n",
      "u\tf\n",
      "t\tt\n",
      "n\tn\n",
      "k\tk\n",
      ">\t>\n",
      " \t \n",
      "a\ta\n",
      "m\tn\n",
      "o\te\n",
      "n\tu\n",
      "g\tg\n",
      " \t \n",
      "t\tt\n",
      "h\th\n",
      "o\te\n",
      "s\tu\n",
      "e\te\n",
      " \t \n",
      "i\tt\n",
      "s\ts\n",
      " \t \n",
      "t\tt\n",
      "h\th\n",
      "e\te\n",
      " \t \n",
      "c\ts\n",
      "o\to\n",
      "m\tm\n",
      "p\tp\n",
      "a\ta\n",
      "n\tn\n",
      "y\ty\n",
      " \t \n",
      "'\t'\n",
      "s\ts\n",
      " \t \n",
      "<\ts\n",
      "u\tu\n",
      "n\tn\n",
      "k\tk\n",
      ">\t>\n",
      " \t \n",
      "i\to\n",
      "n\tn\n",
      "t\t \n",
      "o\te\n",
      " \t \n",
      "t\tt\n",
      "h\th\n",
      "e\te\n",
      " \t \n",
      "p\t<\n",
      "u\tr\n",
      "b\tb\n",
      "l\tl\n",
      "i\ti\n",
      "c\tc\n",
      " \t \n",
      "f\tt\n",
      "a\to\n",
      "c\ti\n",
      "s\tt\n",
      "i\t \n",
      "m\to\n",
      "i\ti\n",
      "l\ts\n",
      "e\tl\n",
      " \t \n",
      "b\t<eos>\n",
      "u\te\n",
      "s\tt\n",
      "i\ti\n",
      "n\tn\n",
      "e\te\n",
      "s\ts\n",
      "s\ts\n",
      " \t \n",
      "m\t<eos>\n",
      "r\ta\n",
      ".\t.\n",
      " \t \n",
      "<\t<\n",
      "u\tu\n",
      "n\tn\n",
      "k\tk\n",
      ">\t>\n",
      " \t \n",
      "s\ta\n",
      "a\ta\n",
      "i\ti\n",
      "d\td\n",
      " \t \n",
      "<eos>\tt\n",
      " \t \n",
      "w\tt\n",
      "i\th\n",
      "t\tt\n",
      "h\th\n",
      "i\t \n",
      "n\tn\n",
      " \tg\n",
      "t\tt\n",
      "o\ti\n",
      "r\tr\n",
      "d\t \n",
      " \t \n",
      "h\tw\n",
      "e\te\n",
      " \t \n",
      "s\ts\n",
      "p\ta\n",
      "o\te\n",
      "k\tk\n",
      "e\te\n",
      " \ts\n",
      "b\tt\n",
      "u\ty\n",
      "t\tt\n",
      " \t \n",
      "a\tt\n",
      "s\t \n",
      " \t \n",
      "a\ta\n",
      " \t \n",
      "g\t<\n",
      "e\tr\n",
      "n\tn\n",
      "e\te\n",
      "r\tr\n",
      "a\ta\n",
      "l\tl\n",
      " \t \n",
      "p\ts\n",
      "r\tr\n",
      "o\to\n",
      "p\tf\n",
      "o\to\n",
      "s\ts\n",
      "i\ta\n",
      "t\tt\n",
      "i\ty\n",
      "o\to\n",
      "n\tn\n",
      " \t \n",
      "i\t<eos>\n",
      " \tn\n",
      "t\t'\n",
      "h\th\n",
      "i\te\n",
      "n\ts\n",
      "k\tk\n",
      " \t \n",
      "w\tt\n",
      "e\th\n",
      " \tl\n",
      "h\t'\n",
      "a\ta\n",
      "v\ts\n",
      "e\te\n",
      " \t \n",
      "a\tt\n",
      " \t \n",
      "v\t<\n",
      "e\te\n",
      "r\tt\n",
      "y\ty\n",
      " \t \n",
      "c\t<\n",
      "o\to\n",
      "n\tm\n",
      "s\tt\n",
      "i\tt\n",
      "s\td\n",
      "t\tt\n",
      "e\ti\n",
      "n\tn\n",
      "t\tt\n",
      " \t \n",
      "t\t<eos>\n",
      "r\to\n",
      "a\ta\n",
      "d\td\n",
      "e\ti\n",
      " \t \n",
      "s\t<eos>\n",
      "t\ta\n",
      "r\to\n",
      "a\te\n",
      "t\tt\n",
      "e\te\n",
      "g\tg\n",
      "y\ti\n",
      " \t \n",
      "i\ta\n",
      "n\tn\n",
      " \t \n",
      "t\tt\n",
      "h\th\n",
      "e\te\n",
      " \t \n",
      "b\t<\n",
      "u\ta\n",
      " \t \n",
      "f\tt\n",
      "o\to\n",
      "u\tr\n",
      "r\tr\n",
      "t\tt\n",
      "h\th\n",
      " \t-\n",
      "q\ta\n",
      "u\tu\n",
      "a\ta\n",
      "r\tr\n",
      "t\tt\n",
      "e\te\n",
      "r\tr\n",
      " \t \n",
      "<eos>\t<eos>\n",
      " \t \n",
      "w\tt\n",
      "e\th\n",
      " \t \n",
      "c\t'\n",
      "o\to\n",
      "u\tn\n",
      "l\tl\n",
      "d\td\n",
      " \t \n",
      "s\th\n",
      "e\tt\n",
      "e\tl\n",
      " \tk\n",
      "h\tt\n",
      "i\te\n",
      "g\ts\n",
      "h\th\n",
      "e\t \n",
      "r\tr\n",
      " \t \n",
      "o\ta\n",
      "i\tf\n",
      "l\tl\n",
      " \t \n",
      "p\t<eos>\n",
      "r\tr\n",
      "i\to\n",
      "c\tc\n",
      "e\te\n",
      "s\ts\n",
      " \t \n",
      "t\t<eos>\n",
      "h\to\n",
      "i\ta\n",
      "s\ts\n",
      " \t \n",
      "y\ti\n",
      "e\te\n",
      "a\ta\n",
      "r\tr\n",
      " \t \n",
      "s\ta\n",
      "a\tt\n",
      "i\ti\n",
      "d\td\n",
      " \t \n",
      "<\tt\n",
      "u\tu\n",
      "n\tn\n",
      "k\tk\n",
      ">\t>\n",
      " \t \n",
      "<\t<\n",
      "u\tu\n",
      "n\tn\n",
      "k\tk\n",
      ">\t>\n",
      " \t \n",
      "a\t<eos>\n",
      "n\tn\n",
      " \td\n",
      "a\t<\n",
      "n\tc\n",
      "a\ta\n",
      "l\tl\n",
      "y\ty\n",
      "s\ts\n",
      "t\tt\n",
      " \t \n",
      "a\t<eos>\n",
      "t\tn\n",
      " \t \n",
      "p\tt\n",
      "a\tr\n",
      "i\tr\n",
      "n\td\n",
      "e\tt\n",
      "w\td\n",
      "e\ti\n",
      "b\tb\n",
      "b\tb\n",
      "e\te\n",
      "i\to\n",
      "a\tb\n",
      "l\tl\n",
      " \t \n",
      "c\ta\n",
      "o\to\n",
      "r\tm\n",
      "p\tp\n",
      ".\t.\n",
      " \t \n",
      "p\t<\n",
      "u\tr\n",
      "r\tb\n",
      "c\tc\n",
      "h\th\n",
      "a\ta\n",
      "s\ts\n",
      "e\te\n",
      "d\t \n",
      " \t \n",
      "t\ta\n",
      "h\th\n",
      "e\te\n",
      " \t \n",
      "b\ts\n",
      "o\ta\n",
      "n\ta\n",
      "d\td\n",
      "s\t \n",
      " \t \n",
      "i\tt\n",
      "n\tn\n",
      " \t \n",
      "a\tt\n",
      "t\t \n",
      " \t \n",
      "l\tt\n",
      "e\te\n",
      "a\ta\n",
      "s\ts\n",
      "t\tt\n",
      " \t \n",
      "n\ta\n",
      " \t \n",
      "d\tn\n",
      "i\ta\n",
      "f\td\n",
      "f\tf\n",
      "e\te\n",
      "r\tr\n",
      "e\ts\n",
      "n\t \n",
      "t\tt\n",
      " \t \n",
      "t\ts\n",
      "r\th\n",
      "a\ta\n",
      "n\td\n",
      "s\ts\n",
      "a\tf\n",
      "c\tc\n",
      "t\tt\n",
      "i\ti\n",
      "o\to\n",
      "n\tn\n",
      "s\t \n",
      " \t \n",
      "i\t<eos>\n",
      "n\tn\n",
      " \t \n",
      "n\tt\n",
      " \t \n",
      "a\tn\n",
      "n\tn\n",
      "d\td\n",
      " \t \n",
      "s\tt\n",
      "i\tt\n",
      "n\tn\n",
      "c\tc\n",
      "e\te\n",
      " \t \n",
      "t\tt\n",
      "h\th\n",
      "e\te\n",
      "n\t \n",
      " \t \n",
      "h\tt\n",
      "a\ta\n",
      "s\ts\n",
      " \t \n",
      "r\tn\n",
      "e\te\n",
      "a\ts\n",
      "l\tl\n",
      "i\tl\n",
      "z\tz\n",
      "e\te\n",
      "d\td\n",
      " \t \n"
     ]
    }
   ],
   "source": [
    "output_array = outputs.cpu().data.numpy()\n",
    "target_array = target.cpu().data.numpy()\n",
    "\n",
    "for j in range(0,10):\n",
    "  for i in range(j,len(output_array),64):\n",
    "    #predict\n",
    "    char_vector = output_array[i]\n",
    "    char = np.argmax(char_vector)\n",
    "    predict_char = TEXT.vocab.itos[char]\n",
    "\n",
    "    # True\n",
    "    char = target_array[i]\n",
    "    true_char = TEXT.vocab.itos[char]\n",
    "    \n",
    "    print(true_char, predict_char, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xHTP_e_LnqJK"
   },
   "source": [
    "# Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xwC0bJZ1h_Sp",
    "outputId": "7978bedb-133a-4952-afe0-2b6bcbe62b05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i think you are n't eatly a <unk> <eos> the state of the state of the state of the state of the state of the state of th\n"
     ]
    }
   ],
   "source": [
    "# Convert text to numeric input\n",
    "input_text = 'i think you are'\n",
    "input_text = list(input_text)\n",
    "\n",
    "text_numeric = []\n",
    "\n",
    "for char in input_text:\n",
    "  text_numeric.append(TEXT.vocab.stoi[char])\n",
    "\n",
    "# Run through model\n",
    "num_generations = 100\n",
    "\n",
    "# Initialize input and hidden\n",
    "input_array = np.array(text_numeric)\n",
    "\n",
    "exp_dim = len(input_array)\n",
    "\n",
    "input_tensor = torch.from_numpy(input_array)\n",
    "input_tensor = input_tensor.expand(1,exp_dim)\n",
    "input_tensor = input_tensor.view(exp_dim,-1)\n",
    "\n",
    "# Run through forward loop\n",
    "outputs = net.forward(input_tensor)\n",
    "\n",
    "# Append predicted to input for next iteration\n",
    "values, indices = outputs[-1].max(0)\n",
    "input_array = np.append(input_array, indices.cpu().numpy())\n",
    "\n",
    "for i in range(num_generations):\n",
    "  # Initialize input and hidden\n",
    "  exp_dim = len(input_array)\n",
    "  input_tensor = torch.from_numpy(input_array)\n",
    "  input_tensor = input_tensor.expand(1,exp_dim)\n",
    "  input_tensor = input_tensor.view(exp_dim,-1)\n",
    "  \n",
    "  # Run through forward loop\n",
    "  outputs = net.forward(input_tensor)\n",
    "\n",
    "  # Append predicted to input for next iteration\n",
    "  values, indices = outputs[-1].max(0)\n",
    "  input_array = np.append(input_array, indices.cpu().numpy())\n",
    "\n",
    "# Print generated sequence\n",
    "predicted_sentence = list() \n",
    "for char in input_array:\n",
    "  predict_char = TEXT.vocab.itos[char]\n",
    "  predicted_sentence.append(predict_char)\n",
    "\n",
    "print(''.join(predicted_sentence))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Gustav_freestyle.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
